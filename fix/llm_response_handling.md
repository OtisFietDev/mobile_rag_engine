# LLM ì‘ë‹µ ì²˜ë¦¬ ë¬¸ì œ í•´ê²°

## ì› ì´ìŠˆ

### ì´ìŠˆ 1: TextResponse("...") í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥
```
TextResponse("I am Gemma, an open-weights AI assistant.")
```

### ì´ìŠˆ 2: Input too long ì—ëŸ¬
```
OUT_OF_RANGE: Input is too long for the model to process:
input_size(723) was not less than maxTokens(512)
```

### ì´ìŠˆ 3: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¯¸ìœ ì§€
"yes"ë¼ê³  ë‹µë³€í•´ë„ ì´ì „ ë§¥ë½ì„ ê¸°ì–µí•˜ì§€ ëª»í•¨

### ì´ìŠˆ 4: LLM ë°˜ë³µ ì¶œë ¥
```
This would allow an aircraft... This would allow an aircraft... This would allow an aircraft...
```

---

## í•´ê²° ê³¼ì •

### ì´ìŠˆ 1 í•´ê²°: TextResponse íŒŒì‹±

**ìˆ˜ì • íŒŒì¼**: `test_app/lib/screens/rag_chat_screen.dart`

```dart
// Extract text from response
String responseText = 'No response generated.';
if (response != null) {
  if (response is TextResponse) {
    // ì§ì ‘ token í”„ë¡œí¼í‹° ì‚¬ìš©
    responseText = response.token;
  } else {
    // Fallback: toString()ì—ì„œ ì¶”ì¶œ
    final raw = response.toString();
    final match = RegExp(r'TextResponse\("(.*)"\)$', dotAll: true).firstMatch(raw);
    if (match != null) {
      responseText = match.group(1) ?? raw;
    } else {
      responseText = raw;
    }
  }
}
```

---

### ì´ìŠˆ 2 í•´ê²°: maxTokens ì¦ê°€

ëª¨ë¸ íŒŒì¼ëª…ì—ì„œ ìš©ëŸ‰ í™•ì¸:
- `Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task`
- `ekv2048` = 2048 í† í° ì§€ì›

```dart
final model = await FlutterGemma.getActiveModel(
  maxTokens: 2048,  // 512 â†’ 2048
  preferredBackend: PreferredBackend.gpu,
);
```

---

### ì´ìŠˆ 3 í•´ê²°: ì˜êµ¬ ì±„íŒ… ì„¸ì…˜

**ë¬¸ì œ**: ë§¤ ë©”ì‹œì§€ë§ˆë‹¤ ìƒˆ ì„¸ì…˜ ìƒì„± â†’ íˆìŠ¤í† ë¦¬ ì†Œì‹¤

**í•´ê²°**: í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ ì„¸ì…˜ ìœ ì§€

```dart
class _RagChatScreenState extends State<RagChatScreen> {
  // LLM ëª¨ë¸ê³¼ ì±„íŒ… ì„¸ì…˜ (ì˜êµ¬ ìœ ì§€)
  InferenceModel? _llmModel;
  InferenceChat? _chatSession;
  
  Future<String> _generateLlmResponse(String query, RagSearchResult ragResult) async {
    // ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
    if (_llmModel == null) {
      _llmModel = await FlutterGemma.getActiveModel(
        maxTokens: 2048,
        preferredBackend: PreferredBackend.gpu,
      );
      _chatSession = await _llmModel!.createChat();
    }
    
    // ê¸°ì¡´ ì„¸ì…˜ì— ë©”ì‹œì§€ ì¶”ê°€
    await _chatSession!.addQueryChunk(Message.text(
      text: prompt,
      isUser: true,
    ));
    
    // ì‘ë‹µ ìƒì„± (ì„¸ì…˜ì´ íˆìŠ¤í† ë¦¬ ìœ ì§€)
    final response = await _chatSession!.generateChatResponse();
    
    // ëª¨ë¸ ë‹«ì§€ ì•ŠìŒ! (ì„¸ì…˜ ìœ ì§€)
    return responseText;
  }
  
  // ì—ëŸ¬ ì‹œ ì„¸ì…˜ ë¦¬ì…‹
  Future<void> _resetChatSession() async {
    if (_llmModel != null) {
      await _llmModel!.close();
      _llmModel = null;
    }
    _chatSession = null;
  }
}
```

---

### ì´ìŠˆ 4 í•´ê²°: ë°˜ë³µ ì¶œë ¥ ì •ë¦¬

ì‘ì€ LLMì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” ë°˜ë³µ ë£¨í”„ ë¬¸ì œ.

```dart
String _cleanRepetition(String text) {
  if (text.length < 100) return text;
  
  // ë¬¸ì¥ ë‹¨ìœ„ ë°˜ë³µ ê°ì§€
  final sentences = text.split(RegExp(r'[.!?\n]'));
  final seenPhrases = <String, int>{};
  final cleanedSentences = <String>[];
  
  for (final sentence in sentences) {
    final key = sentence.trim().toLowerCase();
    final count = (seenPhrases[key] ?? 0) + 1;
    seenPhrases[key] = count;
    
    if (count <= 2) {
      cleanedSentences.add(sentence.trim());
    } else if (count == 3) {
      cleanedSentences.add('...');
      break;  // ë” ì´ìƒì˜ ë°˜ë³µì€ ë¬´ì‹œ
    }
  }
  
  // ë‹¨ì–´ ì‹œí€€ìŠ¤ ë°˜ë³µ ê°ì§€
  // e.g., "This would allow This would allow This would allow"
  // â†’ íŒ¨í„´ 3íšŒ ì´ìƒ ë°˜ë³µ ì‹œ 2íšŒë¡œ ì˜ë¼ëƒ„
  
  return result;
}
```

---

## ì—ëŸ¬ ë¡œê¹… ê°œì„ 

ìƒì„¸ ì—ëŸ¬ëŠ” í„°ë¯¸ë„ì—, ê°„ë‹¨í•œ ë©”ì‹œì§€ëŠ” UIì—:

```dart
} catch (e, stackTrace) {
  // í„°ë¯¸ë„ì— ìƒì„¸ ë¡œê·¸
  debugPrint('ğŸ”´ LLM Error: $e');
  debugPrint('ğŸ”´ Stack Trace: $stackTrace');
  
  // UIì—ëŠ” ê°„ë‹¨í•œ ë©”ì‹œì§€
  String errorType = 'Unknown error';
  if (e.toString().contains('PlatformException')) {
    errorType = 'Model session error';
  } else if (e.toString().contains('OUT_OF_RANGE')) {
    errorType = 'Context too long';
  }
  
  return 'âš ï¸ LLM Error: $errorType\n\n'
         'The model encountered an issue. Please try again.\n'
         '(Check console for details)';
}
```
