# v0.2.0 Update Guide: LLM-Optimized Chunking

This update transforms `mobile_rag_engine` from a simple semantic search tool into a full-fledged RAG (Retrieval-Augmented Generation) pipeline designed for LLMs.

## Why this update matters

### The Problem with v0.1.0
In the previous version, RAG was just "semantic search":
1. You saved a whole document.
2. You searched for it.
3. You got the whole document back.

**Issues:**
- **Token Limits:** If a document is 5,000 words, it likely exceeds the LLM's context window.
- **Diluted Meaning:** Embedding a long document compresses too much information into one vector, making specific queries less accurate.
- **Lost Context:** If you just cut the text at 500 characters, you might cut a sentence in half, confusing the LLM.

### The Solution: Chunking Strategy
v0.2.0 introduces **Recursive Character Chunking with Overlap**, the industry standard for RAG.

## How Chunking Works

### 1. Recursive Splitting
Instead of blindly cutting text every N characters, we try to split at natural boundaries in this order:
1. Double newlines `\n\n` (Paragraphs)
2. Single newlines `\n` (Lines)
3. Sentences `. ` 
4. Words ` `

This ensures that we prefer keeping paragraphs and sentences intact.

### 2. Overlap
We keep a small overlap between chunks to ensure context continuity.

**Example:**
*Input Text:* "Flutter is great. It uses Dart. Dart is type-safe."

*Chunk 1:* "Flutter is great. It uses Dart."
*Chunk 2 (with overlap):* "It uses Dart. Dart is type-safe."

The phrase "It uses Dart" appears in both, creating a "bridge" so the LLM understands the connection.

## New Architecture

### 1. Storage (`Source` vs `Chunk`)
We now separate the **Original Document** from its **Searchable Parts**.

- **Source:** The original full text (e.g., a PDF page, a long article).
- **Chunk:** A small, searchable piece of that source (e.g., a paragraph).

**DB Schema Changes:**
```sql
TABLE sources (id, content, ...)       <-- Holds the full truth
TABLE chunks (source_id, content, ...) <-- Holds vectors for search
```

### 2. Context Assembly
When searching, we don't just return strings. We use `ContextBuilder` to assemble the best possible prompt context within a specific **Token Budget**.

1. **Search** for top-k relevant chunks.
2. **Filter** to make sure we don't exceed the LLM's token limit (e.g., 2000 tokens).
3. **Format** them into a single context string.

## Migration Code Example

### Before (v0.1.0) - Manual & Limited
```dart
// You had to handle everything manually
final text = "Long document...";
final embedding = await EmbeddingService.embed(text);
await addDocument(content: text, embedding: embedding); // Saves whole text!

final results = await searchSimilar(queryEmbedding: q, topK: 5);
// Result: List<String> (Whole documents)
```

### After (v0.2.0) - Automated & Optimized
```dart
// 1. Initialize the new service
final rag = SourceRagService(dbPath: 'rag.db');

// 2. Auto-Chunking & Indexing
// Automatically checks deduplication, splits text, creates embeddings for all chunks
await rag.addSourceWithChunking(
  "Long document...", 
  onProgress: (done, total) => print("Processed $done/$total chunks")
);

// 3. Search & Context Assembly
// "I have 1000 tokens for context, give me the best info"
final result = await rag.search(
  "What is Flutter?", 
  tokenBudget: 1000
);

// 4. Generate LLM Prompt
final prompt = rag.formatPrompt("What is Flutter?", result);

print(prompt);
/* Output:
   Answer based on the following documents:
   
   [Chunk from Doc A] Flutter is a UI toolkit...
   
   [Chunk from Doc B] ...uses Dart for performance.
   
   Question: What is Flutter?
*/
```
