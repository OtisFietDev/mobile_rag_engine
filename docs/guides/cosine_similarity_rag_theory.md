# 코사인 유사도 기반 RAG 이론

로컬 RAG 엔진에서 사용하는 벡터 검색의 핵심 원리를 설명합니다.

---

## RAG란?

**RAG (Retrieval-Augmented Generation)**는 질문에 답변하기 전에 관련 문서를 먼저 검색하여 컨텍스트로 제공하는 방식입니다.

```
[사용자 질문] → [관련 문서 검색] → [LLM에게 문서 + 질문 전달] → [답변]
```

**왜 필요한가?**
- LLM은 학습 이후의 데이터를 모름
- 개인 문서나 사내 정보에 접근 불가
- 검색을 통해 최신/개인 정보를 "주입"할 수 있음

---

## 임베딩 (Embedding)

텍스트를 **고정 길이의 숫자 벡터**로 변환하는 과정입니다.

```
"사과는 맛있다" → [0.12, 0.89, 0.45, ...]  (384~1536차원)
"배는 달콤하다" → [0.14, 0.87, 0.42, ...]  (비슷한 벡터)
"자동차는 빠르다" → [0.91, 0.03, 0.22, ...]  (다른 벡터)
```

**핵심 원리**: 의미가 유사한 문장은 벡터 공간에서 가까운 위치에 배치됨

---

## 코사인 유사도 (Cosine Similarity)

두 벡터 사이의 **각도**를 기반으로 유사성을 측정합니다.

### 수학 공식

```
           A · B
cos(θ) = ─────────
          ‖A‖ × ‖B‖
```

- **A · B**: 내적 (Dot Product)
- **‖A‖, ‖B‖**: 각 벡터의 크기 (L2 Norm)

### 결과 해석

| 값 | 의미 |
|---|---|
| 1.0 | 완전히 동일한 방향 (매우 유사) |
| 0.0 | 직교 (관련 없음) |
| -1.0 | 반대 방향 (반대 의미) |

### 코드로 보기

```rust
// 내적 (Dot Product)
let dot_product = a.dot(&b);  // Σ(aᵢ × bᵢ)

// 크기 (Norm)
let norm_a = a.mapv(|x| x * x).sum().sqrt();  // √Σ(aᵢ²)
let norm_b = b.mapv(|x| x * x).sum().sqrt();

// 코사인 유사도
let similarity = dot_product / (norm_a * norm_b);
```

---

## 벡터 검색 흐름

```
1. [저장 단계]
   문서 → 임베딩 모델 → 벡터 → SQLite에 저장

2. [검색 단계]
   질문 → 임베딩 모델 → 쿼리 벡터
                         ↓
              모든 저장된 벡터와 코사인 유사도 계산
                         ↓
              상위 K개 문서 반환
```

---

## 왜 코사인 유사도인가?

### 다른 거리 측정 방법과 비교

| 방법 | 특징 | 단점 |
|---|---|---|
| **유클리드 거리** | 절대적 거리 측정 | 벡터 크기에 민감 |
| **맨해튼 거리** | 격자 기반 거리 | 고차원에서 부정확 |
| **코사인 유사도** | 방향만 비교 | 크기 무시 |

### 코사인 유사도의 장점

1. **정규화 불필요**: 벡터 크기가 달라도 공정 비교
2. **고차원에 강함**: 임베딩은 보통 384~1536차원
3. **빠른 계산**: 내적과 노름만 계산

---

## 현재 구현의 한계

### Naive Search (전체 순회)

```rust
// 모든 문서를 순회하며 유사도 계산
for row in rows {
    let similarity = calculate_cosine(...);
    candidates.push((similarity, content));
}
```

- **시간 복잡도**: O(N × D) (N: 문서 수, D: 벡터 차원)
- **적합한 규모**: 수백~수천 건

### 대규모 데이터 해결책

| 방법 | 설명 |
|---|---|
| **HNSW** | 그래프 기반 근사 최근접 이웃 |
| **IVF** | 클러스터 기반 인덱싱 |
| **SQLite-VSS** | SQLite 벡터 검색 확장 |

---

## 다음 단계

현재 구현에서는 **벡터를 수동으로 입력**하고 있습니다:

```dart
await addDocument(content: "사과", embedding: [1.0, 0.0, 0.0]);
```

실제 RAG 시스템에서는:

1. **임베딩 모델** (ONNX)을 로드
2. 텍스트를 자동으로 벡터로 변환
3. 저장 및 검색 수행

```dart
// 목표
final embedding = await embedText("사과는 맛있다");  // 모델이 벡터 생성
await addDocument(content: "사과는 맛있다", embedding: embedding);
```
